# 1回归问题
- 目标值为连续型的数据的这一类问题就称为回归问题，解决这一类问题的算法就叫回归算法

# 2线性回归Linear regression
## 2.1定义
- 线性回归 (Linear regression)是利用**回归方程(函数)**对**一个或多个自变量(特征值)**和**因变量(目标值)**之间的关系进行建模的一种分析方法
- **单元回归**: y=kx+b
- **多元回归**:
    - 特点: 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归
- 通用公式: h(w)=w1x1+w2x2+w3x3+...wnxn+b=(w^T)x+b (x1,x2,x3...特征值) (w1,w2,w3...权重值,回归系数) 
- 多元回归，其中w,x可以理解为矩阵，结果为矩阵乘法:  

![线性回归](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.png)  

## 2.2线性回归的特征与目标的关系分析
- **线性回归**当中**线性模型**有两种，一种是**线性关系**，另一种是**非线性关系**
- **线性关系**:**线性关系**一定是**线性模型**。但是**线性模型**不一定是**线性关系**，可能是**非线性关系**
- 我们看特征值与目标值之间建立一个关系，找到最合适的函数(w1,w2,w3...权重值,回归系数，模型参数)以及偏置b，这个关系可以理解为线性模型，这个模型满足线性关系
- **线性关系**:单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面关系 
    - h(w)=w1x1+w2x2+w3x3+...wnxn+b 自变量一次
- **非线性关系**:不能再用一条直线表示 
    - y=w1x1+w2x1**^3**+w4x2**^3**+...+b 参数一次,自变量可以是多次

## 2.3应用场景
- 房价预测
- 销售额度预测
- 金融:贷款额度预测，利用线性回归以及系数分析因子

## 2.4线性回归目标
- 目标：求模型参数，模型参数能够使得预测准确 

![损失](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8D%9F%E5%A4%B1.png)

## 2.5损失函数/cost/成本函数/目标函数
![损失函数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png) 
- hw(xi)为第i个训练样本特征值组合预测函数/预测结果
- yi为第i个真实值
- 又称为最小二乘法
- 衡量标准:均方误差

## 2.6优化算法正规方程和梯度下降
- 缩小损失，使得损失函数最小
- 如果求出模型当中的wi，使得损失最小
- 有两种优化算法
- 正规方程
- 梯度下降

### 2.6.1正规方程  
    
![正规方程](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png)
- 理解: x为特征值矩阵，y为目标值矩阵，**直接**求到最好的结果
    - （特征值矩阵的转置*特征值矩阵)求逆*特征值矩阵的转置*目标值矩阵
- 缺点: 当特征过多或过复杂时，求解速度太慢并且得不到结果

### 2.6.2梯度下降 Gradient Descent  

![梯度下降](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png)

![梯度下降图](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE.png)

- α为学习速率，需要手动定值(超参数)，α旁边的整体表示方向，沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新w值使用；面对训练数据模型十分庞大的任务，能够找到很好的结果
- 不仅仅在线性回归会用，逻辑回归和深度学习都会用到梯度下降，适用的范围比较广

## 2.7API优化算法
- **from sklearn.linear_model**

### 2.7.1**LinearRegression**正规方程优化
- **sklearn.linear_model.LinearRegression(fit_intercept=True)**
    - fit_intercept: 是否计算**偏置**
- **LinearRegression.coef_:回归系数**
- **LinearRegression.intercept:偏重**

### 2.7.2**SGDRegressor**随机梯度下降学习
- **sklearn.linear_model.SGDRegressior(loss='squared_loss',fit_intercept=True,learning_rate='invscaling',eta0=0.01)**
- 支持不同的**loss函数**和**正则化惩罚**来拟合线性回归模型    
- loss:损失类型
    - loss='squared_loss':普通最小二乘法
- fit_intercept:是否计算**偏值**
- learning_rate:string,optional 默认为'invscaling'
    - 学习率填充/步长
    - 'constant':eta=eta()  默认的是0.01 学习率保持在一个数据不变
    -  'optimal':eta=1.0/(alpha*(t+t0))[default]
    -  'invscaling':eta=eta0/pow(t,power_t) 默认下降的步长invscaling
        - power_t=0.25:存在于父类当中
    - 对于一个常数值的学习率来说，可以使用learning_rate='constant', 并使用eta0来指定学习率
- **SGDRegressor.coef_: 回归系数**
- **SGDRegressor.intercept_:偏置**


## 2.8模型评估方法

### 2.8.1回归当中的数据大小不一致，是否为导致结果影响较大，所以需要做标准化处理
- 数据分割与标准化处理
- 回归预测
- 线性回归的**算法效果评估**

### 2.8.2回归性能评估
- 均方误差(Mean Squared Error MSE)评价机制

![模型评估方法](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.png)

### 2.8.3API均方误差
- **sklearn.metrics.mean_squared_erroy(y_true,y_pred)**
- 均方误差回归损失
- y_true:真实值
- y_pred:预测值
- return:浮点数结果

## 2.9正规方程和梯度下降对比
正规方程:
- 不需要选择学习率
- 不需要迭代求解
- 需要计算方程，时间复杂度高O(n3)
- 局限性强，**容易过拟合**

梯度下降  
- 需要选择学习率
- 需要迭代求解
- 特征数量较大可以使用

选择:  
- 小规模数据:
    - LinearRegression(正规方程不能解决拟合问题)
    - 岭回归
- 大规模数据: SGDRegressor随机梯度下降学习

## 2.10梯度下降的一些优化方法GD,SGD,SAG
- 另外一些优化算法
- GD
    - 梯度下降(Gradient Descent), 原始的梯度下降法需要计算**所有样本的值**才能够得出梯度，计算量大，所以后面才会有一系列的改进
- SGD
    - **随机梯度下降(Stochatic gradient descent)是一个优化方法，他在一次迭代时只考虑一个训练样本**
    - SGD的优点
        - 高效
        - 容易实现
    - SGD的缺点
        - SGD需要许多超参数，比如正则项参数，迭代数
        - SGD对于特征标准化是敏感的
- SAG
    - 随机平均梯度(Stochasitc Average Gradient), 由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法

Scikit-learn:岭回归，逻辑回归等当中都会有SAG优化


## 2.11案例波士顿房价预测
- 数据集介绍,获取数据集
- 划分数据集
- 特征工程
    - 无量纲化:标准化
- 预估器流程
    - 可以调整学习率
    - fit()--->模型
        - **estimator.coef_**
        - **estimator.intercept_**
```
实例数量:506
属性数量: 13数值型或类别型，帮助预测的属性；中心位(第14个属性) 经常是学习的目标
属性信息(按顺序)
- CRIM 城镇人均犯罪率 连续值
- ZN 占地面积超过2.5万平方英尺的住宅用地比例 连续值
- INDUS 城镇非零售业务地区的比例 连续值
- CHAS 查尔斯河虚拟变量(=1如果土地在河边；否则为0)，是否邻近查尔斯河 离散值，1=邻近；0=不邻近
- NOX 一氧化氮浓度(每1000万份) 连续值
- RM 每栋房屋的平均客房数 连续值
- AGE 在1940年之前建成的自用单位比例 连续值
- DIS 与五个波士顿就业中心的加权距离 连续值
- RAD 辐射状公路的可达性指数 连续值
- TAX 每10000美元的全额物业税率，全值财产税收 连续值
- PTRATIO 城镇师生比例 连续值
- B 1000(Bk-0.63)^2 其中Bk是城镇的黑人比例 连续值
- LSTAT 低收入人群占比 连续值
- MEDV 同类房屋价格的中位数 连续值

缺失属性值： 无
创建者：Harrison D andRubinfeld D,L
```

```python
from sklearn.datasets import  load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression,SGDRegressor
from sklearn.metrics import mean_squared_error
def linear_demo1():
    '''
    正规方程的优化方法对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程
    estimator = LinearRegression()
    estimator.fit(x_train,y_train)
    # 得出模型
    print("正规方程权重系数为：\n", estimator.coef_)
    print("正规方程偏置为:\n",estimator.intercept_)
    y_predict = estimator.predict(x_test)
    print("正规方程预测房价：\n", y_predict)
    #模型评价:均方误差
    error = mean_squared_error(y_test,y_predict)
    print("正规方程均方误差为:\n",error)
    return None


def linear_demo2():
    '''
    梯度下降的优化方法对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程, 学习率的算法constant，eta0=0.001 迭代次数10000
    estimator = SGDRegressor(learning_rate="constant",eta0=0.01,max_iter=10000)
    estimator.fit(x_train,y_train)
    # 得出模型
    print("梯度下降权重系数为：\n", estimator.coef_)
    print("梯度下降偏置为:\n",estimator.intercept_)
    y_predict = estimator.predict(x_test)
    print("梯度下降预测房价：\n", y_predict)
    #模型评价:均方误差
    error = mean_squared_error(y_test, y_predict)
    print("梯度下降均方误差为:\n", error)
    return None

if __name__ == "__main__":
    linear_demo1()
    linear_demo2()
    
output:
正规方程权重系数为：
 [-0.63330277  1.14524456 -0.05645213  0.74282329 -1.95823403  2.70614818
 -0.07544614 -3.29771933  2.49437742 -1.85578218 -1.7518438   0.8816005
 -3.92011059]
正规方程偏置为:
 22.6213720317
正规方程预测房价：
 [ 28.15624208  31.30869316  20.51485702  31.48205292  19.01722351
  18.25171434  20.57503703  18.45503556  18.46192151  32.94820922
  20.36213103  27.24752425  14.81963448  19.21146435  37.02505033
  18.32408346   7.70119888  17.56478207  30.19561854  23.61297215
  18.13379616  33.84017096  28.49921616  16.99629682  34.76148752
  26.227388    34.84170356  26.6267998   18.63962161  13.21549955
  30.36603792  14.70412444  37.18508975   8.91445391  15.06484067
  16.12468763   7.21797311  19.16335583  39.57444328  28.24501235
  24.62961494  16.72956407  37.82734499   5.70546434  21.20919004
  24.63811904  18.85963528  19.93919917  15.20065511  26.3036171
   7.4251188   27.14868579  29.19076714  16.28206033   7.94953105
  35.46279456  32.39096932  20.83555382  16.41378444  20.87373635
  22.92853043  23.61293997  19.32937197  38.34148716  23.87879591
  18.96954218  12.59209375   6.13512682  41.45864696  21.09486655
  16.23896752  21.48997696  40.7412586   20.4923302   36.81939833
  27.05431089  19.80309379  19.61594823  24.59557969  21.0926586
  30.92608611  19.33654808  22.30425056  31.09257529  26.36682634
  20.25040256  28.82330164  20.82975275  26.02088244  19.38265499
  24.96346722  22.30487912  18.92534649  18.86319188  14.02247729
  17.42627701  24.19757886  15.83147538  20.07623475  26.5274431
  20.1203599   17.01175154  23.87970119  22.84994222  21.01501787
  36.18004225  14.68047932  20.5703347   32.46950515  33.24267189
  19.81863526  26.56674006  20.90143053  16.41628584  20.76797132
  20.55412335  26.86514583  24.14833578  23.24521672  13.80658275
  15.37015731   2.78305654  28.90480222  19.78978857  21.50300566
  27.54836226  28.55537501]
正规方程均方误差为:
 20.0616866377
梯度下降权重系数为：
 [-1.1367301   1.29981158  0.01371703  0.70522691 -1.87924507  3.02783621
 -0.13158741 -3.35524493  2.51781459 -1.98037893 -2.10577075  0.74408847
 -4.14145192]
梯度下降偏置为:
 [ 22.98278092]
梯度下降预测房价：
 [ 28.79591589  32.46529121  20.6972206   32.27031095  19.02129416
  17.95845514  20.69062811  18.46059213  18.56479906  34.66701464
  20.58748893  27.41442739  14.47701213  19.29277326  39.1825514
  18.24197237   7.99499827  17.3565987   31.39513908  24.10611464
  17.94206754  35.88374581  29.30872993  16.11266615  36.13360925
  26.84557962  36.50572459  27.8286162   17.93750467  13.21106655
  32.18228073  14.72516264  39.61450296   6.1218561   14.69429992
  15.67196206   4.75423195  18.81753535  41.53287574  29.19937696
  25.09020445  16.64918092  39.21170216   4.03874703  20.99464407
  25.20875726  19.57341391  20.07851485  14.61924293  26.06095451
   6.83551472  27.94966016  29.86064671  15.09612234   7.58621584
  37.58702537  33.67752148  21.42633293  16.47071909  21.29911939
  23.28304646  23.8337421   19.28305196  40.44993115  24.65416451
  18.70292604  12.56611894   3.97841096  43.70120616  21.35786865
  16.15306576  21.87778958  43.4805356   20.76593801  39.11440314
  27.69318539  20.34802499  20.21777059  25.34028844  21.97115867
  32.01543659  19.56686171  22.4464336   33.0804518   27.12579609
  20.00946688  29.68493871  21.17930608  26.39383785  19.69575129
  26.03327167  22.35866282  18.904526    15.39559851  14.20725576
  17.27986265  24.66620958  15.75625322  19.83976949  27.05527824
  19.79054531  16.61507865  24.76015783  23.19264809  21.42980927
  37.45522173  14.32648618  21.014171    34.06632711  33.50908983
  20.01364528  27.32089996  22.03374396  16.37183233  20.89150303
  21.60194373  28.33491462  25.38779336  23.63872617  12.92319984
  14.45680619   0.80002118  29.81417126  19.77184892  21.75293765
  28.27667111  29.81715417]
梯度下降均方误差为:
 19.9206057374
```

