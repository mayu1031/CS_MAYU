# 决策树

## 1什么是决策树

- 决策树的构造
    - 优点: 计算复杂度度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
    - 缺点: 可能会产生过度匹配问题
    - 适用数据类型: 数值型和标称型

- 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)
    - 分类型可以分为无序和有序
    - 无序分类的离散化(港口ABC)，每个值映射为一个数字，比如C=0, Q=1, S=2。 但是这样容易产生一个问题：我们实际上是把它们当做了有序的数字来进行看待了，2比1大，这就存在了顺序关系。但是我们的数据本来并不存在这样的关系。为了解决上面的问题，我们使用独热编码（One-Hot Encoding）对无序的分类变量进行处理。
    - 有序分类变量的离散化，有序分类变量可以直接利用划分后的数值。如分类变量 [贫穷，温饱，小康，富有]，直接可以将他们转换为[0,1,2,3]就可以了。可以直接使用pandas当中的map函数进行映射离散化，或者是借用sklearn.preprocessing.LabelEncoder 来完成这样的操作。
```
train_df['Sex'] = train_df['Sex'].map({'male':0, 'female':1})
```

- 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等(数值型目标变量主要用于回归分析)
    - 离散型和连续型
    - 对于连续变量，我们直接把他们扔进我们的模型当中，为什么还有进行离散化？
    - 离散化有很多的好处，比如能够使我们的模型更加的简单，因为相对于连续类型数据，离散类型数据的可能性更少。对于某些模型比如计算广告中常用的逻辑回归，是非常需要我们输入离散化的特征的。
    - 连续特征离散化的方法可以分为有监督的和无监督的。前者主要是利用了数据集中的类信息。
    - 无监督的方法分为：
        - 等宽划分：按照相同宽度将数据分成几等份。缺点是受到异常值的影响比较大。 pandas.cut方法可以进行等宽划分。
        - 等频划分：将数据分成几等份，每等份数据里面的个数是一样的。pandas.qcut方法可以进行等频划分。
        - 聚类划分：使用聚类算法将数据聚成几类，每一个类为一个划分。

-  构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到**决定性的特征**，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分成几个数据子集。这些数据子分布在第一决策点的所有分支上。如果某个分支下的数据属于同一个类型，则当前分支下已经正确地数据分类，无需进一步对数据集进行划分。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程，
    - createBranch 是一个递归函数

```
创建分支的伪代码函数 createBranch()
检测数据集中的每个子项是否属于同一分类
IF so return 类标签；
else
    寻找划分数据集的最好特征
    划分数据集
    创建分支节点
        for 每个划分的子集
            调用函数 createBranch 并增加返回结果到分支节点中
    return 分支节点
```

- 决策树的一般流程
    - 收集数据
    - 准备数据: 数构造算法只适用于标称型数据，所以数值型数据必须离散化
    - 分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期
    - 训练方法: 构造树的数据结构
    - 测试算法: 使用经验数计算错误率
    - 使用方法

- 离散化: 把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率
    - 通俗的说，离散化是在不改变数据相对大小的条件下，对数据进行相应的缩小
    - 当数据只与它们之间的相对大小有关，而与具体是多少无关时，可以进行离散化
    
- 决策树: 从根节点开始一步步走到叶子节点
    - 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归
    - 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构
- 数的组成:
    - 根节点:第一个选择点
    - 非叶子节点与分支:中间过程
    - 叶子节点:最终的决策结果      
    
- 目标:通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，以此类推
    - 高效的进行决策
    - 快速的确定特征的先后顺序
    

## 2决策树分类原理衡量标准
- 划分数据集的最大原则，将无序的数据变得更加有序

### 2.1信息熵(entropy)
- 熵是表示随机变量不确定性的度量
    - H的专业术语称为信息熵，单位为比特bit
    - 不确定越大，得到的熵值也就越大
    - p(x)的范围在0-1之间，当p(x)=1时，logp(x)=0，H(x)=0，当p(x)=0时，H(x)=0，随机变量完全没有不确定性，在p(x)越接近1或0时，H(x)越接近0
    - 当p=0.5时，H(x)=1，此时随机变量的不确定性最大，熵值也是最大的

- 熵定义为信息的期望值
    - 什么是信息，如果待分类的事务可以划分在多个分类之中，则其中符号xi(分类xi)的信息定义l(xi)=-log2p(xi)，其中p(xi)为选择该分类xi的概率
    - 为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值
    - 公式:
     
 ![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png)    
    

### 2.2信息熵性质
- 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太 阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
- 非负性，即信息熵不能为负。即你得知了某个信息后，却增加了不确定性是不合逻辑的。
- 累加性，即多随机事件同时发生的总不确定性的量度是可以表示为各事件不确定性的量度的和。

### 2.3信息增益(information gain)
- 在划分数据集之前之后信息发生的变化称为信息增益
- 决策树的划分依据之一
- 表示特征A使得数据集D的不确定性减少的程度
- 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征A的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式:  
    
![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png)  


## 3案例  
- 在历史数据中，14天里面有9天打球，5天不打球，此时的熵值:-(9/14)log2(9/14)-(5/14)long2(5/14)=0.940  
- 此数据中一共有4个特征，我们先分析outlook,outlook有sunny，overcast，rainy三种:
- sunny时，sunny有5天，其中出去两天，没出去三天: -(2/5)log2(2/5)-(3/5)log2(3/5)=0.971
- overcast 0
- rainy 0.971
- 此时的熵值: 0.971*(5/14)+0*(4/14)+0.971*(5/14)=0.693
- 信息增益: 0.940-0.693=0.247
- 另外三个特征的增加分别为0.029，0.152，0.048


## 4决策树分类原理其他方法
- ID3 
    - 信息增益，最大准则
- C4.5
    - 信息增益率 解决ID3问题，考虑自身熵信息 增益/自身熵值 最大准则
- CART
    - 分类树: GINI基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致  
    - 公式:  
    
![GINI基尼系数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/GINI%E7%B3%BB%E6%95%B0.png)

## 5连续值划分
- 之前获取的数值都是离散值，或者按属性分；对于连续值最好的处理方法，是进行'二分'，找到离散点进行切分

## 6决策树剪枝策略
- 为什么要剪枝: 决策树过拟合风险很大，泛化能力太差，理论上可以完全分得开数据
- 剪枝策略: 预剪枝，后剪枝
- 预剪枝: 边建立决策树边进行剪枝的操作，限制参数 更实用
    - 限制深度，叶子节点个数，叶子节点样本数，信息增益量等
- 后剪枝: 当建立完决策树后来进行剪枝操作


## 7决策树API
- **from sklearn import tree**
- **sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)**
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子，是在任意带有随机性的类或函数里作为参数来控制随机模式。

## 8树模型参数：
- 1.**criterion: gini or entropy**决策树的**划分依据**, 默认是'gini'系数，也可以选择信息增益的熵'entropy'

- 2.splitter: best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）默认best

- 3.max_features: 默认是None（即所有），log2，sqrt，N 特征小于50的时候一般使用所有的

- 4.**max_depth**: **深度**，数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下 max_depth=2 只选择最好的两个特征来建立模型

- 5.**min_samples_split**: **叶子节点样本数**，如果某节点的**样本数**少于min_samples_split，则不会继续再尝试选择最优特征来进行划分；如果样本量不大，不需要管这个值，如果样本量数量级非常大，则推荐增大这个值

- 6.min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被**剪枝**，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

- 7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本**权重**和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了

- 8.max_leaf_nodes 通过**限制最大叶子节点数**，可以**防止过拟合**，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到

- 9.class_weight 指定样本**各类别的的权重**，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高

- 10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 

- 11.n_estimators:要建立**树的个数**

## 9鸢尾花案例

```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

## 10决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```python
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```
```
output:

digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

## 11模型选择和调优
### 11.1超参数搜索-网格搜索(Grid Search)
超参数 通常情况下，参数是需要手动指定的(如k-近邻算法中的k值)，这种叫超参数  

但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
```
k值   k=3   k=5   k=7
模型 模型1 模型2 模型3
```
### 11.2模型选择与调优API
- **sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)**
- 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
        - cv=3 训练集分成三分，其中一份测试，一共训练三次
- fit():输入训练数据
    - estimator.fit(x_train,y_train)
- predict():预测测试集结果
    - y_predict = estimator.predict(x_test)
    - print("y_predict:\n", y_predict)
- score():准确率
    - score = estimator.score(x_test,y_test)
    - print("准确率：\n",score)
- 结果分析:
    - 最佳参数: best_params_
    - 最佳结果: best_score_
    - 最佳估计器: best_estimator_
    - 交叉验证结果: cv_results_
    - p=1是曼哈顿距离，p=2是欧式距离
    

## 12案例网格搜索(Grid Search)
```python
# 模型调优与选择
# 超参数搜索-网格搜索(Grid Search) GridSearchCV
# param_grid 为参数；参数候选项写成字典的格式
# cv 进行几次交叉验证

from sklearn.grid_search import GridSearchCV
tree_param_grid = { 'min_samples_split': list((3,6,9)),'n_estimators':list((10,50,100))}
grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid, cv=5)
grid.fit(x_train, y_train)
grid.grid_scores_, grid.best_params_, grid.best_score_
```
```
output:
([mean: 0.78405, std: 0.00505, params: {'min_samples_split': 3, 'n_estimators': 10},
  mean: 0.80529, std: 0.00448, params: {'min_samples_split': 3, 'n_estimators': 50},
  mean: 0.80673, std: 0.00433, params: {'min_samples_split': 3, 'n_estimators': 100},
  mean: 0.79016, std: 0.00124, params: {'min_samples_split': 6, 'n_estimators': 10},
  mean: 0.80496, std: 0.00491, params: {'min_samples_split': 6, 'n_estimators': 50},
  mean: 0.80671, std: 0.00408, params: {'min_samples_split': 6, 'n_estimators': 100},
  mean: 0.78747, std: 0.00341, params: {'min_samples_split': 9, 'n_estimators': 10},
  mean: 0.80481, std: 0.00322, params: {'min_samples_split': 9, 'n_estimators': 50},
  mean: 0.80603, std: 0.00437, params: {'min_samples_split': 9, 'n_estimators': 100}],
 {'min_samples_split': 3, 'n_estimators': 100},
 0.8067250881273065)
```

- 带入min_samples_split': 3, 'n_estimators': 100测试
```python
rfr = RandomForestRegressor( min_samples_split=3,n_estimators = 100,random_state = 42)
rfr.fit(data_train, target_train)
rfr.score(data_test, target_test)
```
```
output:
0.80908290496531576 
```

## 13流程分析
- 获取数据
- 数据处理
    - 特征值 x
    - 目标值 y
- 特征工程：标准化
- 算法预估流程
- 模型选择与调优
- 模型评估
