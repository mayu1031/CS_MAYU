

- 离散化: 把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率
    - 通俗的说，离散化是在不改变数据相对大小的条件下，对数据进行相应的缩小
    - 当数据只与它们之间的相对大小有关，而与具体是多少无关时，可以进行离散化

## 对数据分类
    
- 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)
    - 分类型可以分为无序和有序
    - 无序分类的离散化(港口ABC)，每个值映射为一个数字，比如C=0, Q=1, S=2。 但是这样容易产生一个问题：我们实际上是把它们当做了有序的数字来进行看待了，2比1大，这就存在了顺序关系。但是我们的数据本来并不存在这样的关系。为了解决上面的问题，我们使用独热编码（One-Hot Encoding）对无序的分类变量进行处理。
    - 有序分类变量的离散化，有序分类变量可以直接利用划分后的数值。如分类变量 [贫穷，温饱，小康，富有]，直接可以将他们转换为[0,1,2,3]就可以了。可以直接使用pandas当中的map函数进行映射离散化，或者是借用sklearn.preprocessing.LabelEncoder 来完成这样的操作。
```
train_df['Sex'] = train_df['Sex'].map({'male':0, 'female':1})
```

- 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等(数值型目标变量主要用于回归分析)
    - 离散型和连续型
    - 对于连续变量，我们直接把他们扔进我们的模型当中，为什么还有进行离散化？
    - 离散化有很多的好处，比如能够使我们的模型更加的简单，因为相对于连续类型数据，离散类型数据的可能性更少。对于某些模型比如计算广告中常用的逻辑回归，是非常需要我们输入离散化的特征的。
    - 连续特征离散化的方法可以分为有监督的和无监督的。前者主要是利用了数据集中的类信息。
    - 无监督的方法分为：
        - 等宽划分：按照相同宽度将数据分成几等份。缺点是受到异常值的影响比较大。 pandas.cut方法可以进行等宽划分。
        - 等频划分：将数据分成几等份，每等份数据里面的个数是一样的。pandas.qcut方法可以进行等频划分。
        - 聚类划分：使用聚类算法将数据聚成几类，每一个类为一个划分。




    


